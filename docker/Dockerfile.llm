FROM 3x3cut0r/llama-cpp-python:latest

ENV MODEL_PATH='/models/model.gguf' \
    MODEL_URL='https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-GGUF/resolve/main/Phi-3-mini-4k-instruct-q4.gguf'

RUN mkdir -p /models && \
    if [ ! -f "$MODEL_PATH" ]; then \
    wget -O "$MODEL_PATH" "$MODEL_URL" \
    fi
ENTRYPOINT sh -c '/llama.cpp/server -m "$MODEL_PATH" --ctx-size 2048 --threads 4 --batch-size 256'
